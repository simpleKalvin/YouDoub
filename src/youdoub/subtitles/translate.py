from __future__ import annotations

from pathlib import Path
from typing import List, Dict, Optional
import re
from ..utils.llm_adapters import get_translator
from ..utils.logging import get_logger

logger = get_logger(__name__)


DEFAULT_PROMPT = "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º ã€ç›®æ ‡è¯­è¨€ã€‘ã€‚æ–‡ä¸­æ¶‰åŠå‘éŸ³ç¤ºä¾‹ã€ä¸“æœ‰åè¯æˆ–è‹±æ–‡åŸè¯çš„éƒ¨åˆ†è¯·ä¿æŒè‹±æ–‡åŸæ ·ï¼Œä¸è¦ç¿»è¯‘,åªç¿»è¯‘å…¶ä»–èƒ½ç¿»è¯‘çš„éƒ¨åˆ†ï¼Œæ–‡æœ¬ï¼šã€æ–‡æœ¬ã€‘"

# æ”¹è¿›çš„æç¤ºè¯ï¼Œä¸“é—¨ç”¨äºå­—å¹•ç¿»è¯‘ï¼Œç¡®ä¿è¯­åºé¡ºç•…
SUBTITLE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­—å¹•ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡å­—å¹•ç¿»è¯‘æˆã€ç›®æ ‡è¯­è¨€ã€‘ã€‚

ç¿»è¯‘è¦æ±‚ï¼š
1. ä¿æŒè‡ªç„¶æµç•…çš„è¯­åºï¼Œä¸è¦é€å¥ç›´è¯‘
2. é€‚å½“è°ƒæ•´å¥å­ç»“æ„ä»¥ç¬¦åˆã€ç›®æ ‡è¯­è¨€ã€‘çš„è¡¨è¾¾ä¹ æƒ¯
3. ä¿ç•™åŸæ„ï¼Œä½†å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡é€‚å½“è°ƒæ•´è¡¨è¾¾æ–¹å¼
4. ä¸“ä¸šæœ¯è¯­ã€ä¸“æœ‰åè¯ã€äººåã€åœ°åä¿æŒè‹±æ–‡åŸæ ·
5. è¯­æ°”å’Œé£æ ¼è¦ä¸åŸæ–‡ä¿æŒä¸€è‡´

åŸæ–‡å­—å¹•å†…å®¹ï¼š
ã€æ–‡æœ¬ã€‘

è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘åçš„å­—å¹•å†…å®¹ï¼Œä¿æŒç›¸åŒçš„æ®µè½ç»“æ„ã€‚"""

# ä¸“ç”¨çš„ SRT æ¨¡å¼æç¤ºè¯ï¼šæ˜ç¡®å‘Šè¯‰æ¨¡å‹è¾“å…¥æ˜¯å®Œæ•´çš„ SRT æ–‡ä»¶å¹¶è¦æ±‚è¿”å›æœ‰æ•ˆçš„ SRT
SUBTITLE_PROMPT_SRT = """ä½ å°†æ”¶åˆ°ä¸€ä¸ªå®Œæ•´çš„ SRT å­—å¹•æ–‡ä»¶å†…å®¹ï¼ˆindex, timestamp, textï¼‰ã€‚
å°†å­—å¹•æ–‡æœ¬ç¿»è¯‘ä¸ºã€ç›®æ ‡è¯­è¨€ã€‘ï¼Œå¹¶è¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„ SRT æ–‡ä»¶ï¼š
- ä¿æŒæ‰€æœ‰ç´¢å¼•å’Œæ—¶é—´æˆ³å®Œå…¨ä¸å˜ã€‚
- åªæ›¿æ¢æ¯ä¸ªæ¡ç›®çš„æ–‡æœ¬è¡Œä¸ºé€šé¡ºã€è‡ªç„¶çš„è¯‘æ–‡ã€‚
- ä¸è¦æ·»åŠ ã€åˆ é™¤æˆ–é‡ç¼–å·æ¡ç›®ã€‚
- ä¿ç•™åŸæ–‡ä¸­çš„ä¸“æœ‰åè¯æˆ–è‹±æ–‡åŸè¯ï¼ˆå½“é€‚å½“æ—¶ï¼‰ã€‚
åªè¿”å›ç¿»è¯‘åçš„ SRT å†…å®¹ï¼ˆä¸è¦æ·»åŠ è§£é‡Šæˆ–æ³¨é‡Šï¼‰ã€‚

SRT_INPUT:
ã€æ–‡æœ¬ã€‘
"""


def parse_srt(path: Path) -> List[Dict]:
    text = path.read_text(encoding="utf-8", errors="ignore")
    parts = [p.strip() for p in text.split("\n\n") if p.strip()]
    out = []
    for part in parts:
        lines = part.splitlines()
        if len(lines) < 2:
            continue
        idx = lines[0].strip()
        times = lines[1].strip()
        body = "\n".join(l.rstrip() for l in lines[2:]).strip()
        if "-->" not in times:
            continue
        start, end = [t.strip() for t in times.split("-->")]
        out.append({"index": idx, "start": start, "end": end, "text": body})
    return out


def write_srt(path: Path, entries: List[Dict]) -> None:
    parts = []
    for e in entries:
        parts.append(str(e["index"]))
        parts.append(f"{e['start']} --> {e['end']}")
        parts.append(e["text"])
        parts.append("")
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text("\n".join(parts), encoding="utf-8")


def batch_entries(entries: List[Dict], max_chars: int = 1000, max_items: int = 10) -> List[List[Dict]]:
    batches: List[List[Dict]] = []
    cur: List[Dict] = []
    cur_chars = 0
    for e in entries:
        length = len(e.get("text", "")) + 1
        if cur and (cur_chars + length > max_chars or len(cur) >= max_items):
            batches.append(cur)
            cur = []
            cur_chars = 0
        cur.append(e)
        cur_chars += length
    if cur:
        batches.append(cur)
    return batches


def timestamp_to_ms(timestamp: str) -> int:
    """Convert SRT timestamp to milliseconds"""
    # Format: "00:00:01,234 --> 00:00:02,567"
    hours, minutes, seconds = timestamp.split(":")
    seconds, milliseconds = seconds.split(",")
    return (int(hours) * 3600 + int(minutes) * 60 + int(seconds)) * 1000 + int(milliseconds)


def ms_to_timestamp(ms: int) -> str:
    """Convert milliseconds to SRT timestamp format"""
    hours = ms // 3600000
    ms %= 3600000
    minutes = ms // 60000
    ms %= 60000
    seconds = ms // 1000
    milliseconds = ms % 1000
    return "02d"


def merge_short_entries(entries: List[Dict], min_duration_ms: int) -> List[Dict]:
    """Merge entries that are shorter than min_duration_ms"""
    if not entries:
        return entries

    merged = []
    current_group = []

    for entry in entries:
        start_ms = timestamp_to_ms(entry["start"])
        end_ms = timestamp_to_ms(entry["end"])
        duration = end_ms - start_ms

        if duration < min_duration_ms:
            # å¦‚æœå½“å‰æ¡ç›®å¤ªçŸ­ï¼ŒåŠ å…¥åˆ°å½“å‰ç»„
            current_group.append(entry)
        else:
            # å¦‚æœå½“å‰æ¡ç›®å¤Ÿé•¿ï¼Œå…ˆå¤„ç†ä¹‹å‰ç´¯ç§¯çš„çŸ­æ¡ç›®ç»„
            if current_group:
                merged.append(merge_entry_group(current_group))
                current_group = []
            # æ·»åŠ å½“å‰æ¡ç›®
            merged.append(entry)

    # å¤„ç†æœ€åçš„çŸ­æ¡ç›®ç»„
    if current_group:
        merged.append(merge_entry_group(current_group))

    return merged


def merge_entry_group(group: List[Dict]) -> Dict:
    """Merge a group of entries into one"""
    if not group:
        return None
    if len(group) == 1:
        return group[0]

    # åˆå¹¶æ–‡æœ¬
    texts = [entry["text"] for entry in group]
    merged_text = " ".join(texts)

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¡ç›®çš„å¼€å§‹æ—¶é—´å’Œæœ€åä¸€ä¸ªæ¡ç›®çš„ç»“æŸæ—¶é—´
    start_time = group[0]["start"]
    end_time = group[-1]["end"]
    index = group[0]["index"]

    return {
        "index": index,
        "start": start_time,
        "end": end_time,
        "text": merged_text
    }


def split_translated_text(translated: str, n: int, originals: List[Dict]) -> List[str]:
    # Try simple newline split first
    lines = [ln.strip() for ln in translated.splitlines() if ln.strip()]
    if len(lines) == n:
        # ç§»é™¤æ¯è¡Œå¼€å¤´çš„åºå·
        cleaned_lines = []
        for line in lines:
            cleaned = re.sub(r'^\s*\[?\d+\]\s*', '', line)
            cleaned = re.sub(r'^\s*\d+\s*[.ã€)]\s*', '', cleaned)
            cleaned_lines.append(cleaned.strip())
        return cleaned_lines

    # Fallback: split by sentence punctuation while preserving approximate lengths
    # naive sentence split
    sents = re.split(r'(?<=[ã€‚.!?ï¼ï¼Ÿ])\s*', translated.strip())
    sents = [s for s in sents if s.strip()]
    if len(sents) >= n:
        # group sentences into n buckets by length
        buckets = [[] for _ in range(n)]
        lens = [len(o.get("text","")) for o in originals]
        total = sum(lens) or 1
        # desired proportion per bucket
        proportions = [l/total for l in lens]
        # assign sentences greedily to match proportions
        idx = 0
        for sent in sents:
            buckets[idx].append(sent)
            # move idx forward sometimes
            idx = (idx + 1) % n
        return [" ".join(b) for b in buckets]

    # As last resort, split translated string into n almost-equal chunks by characters
    translated = translated.strip()
    L = len(translated)
    if L == 0:
        return ["" for _ in range(n)]
    chunk_size = max(1, L // n)
    out = [translated[i*chunk_size:(i+1)*chunk_size].strip() for i in range(n)]
    # append remainder to last
    if n*chunk_size < L:
        out[-1] += translated[n*chunk_size:].strip()
    return out


def translate_srt_file(
    input_path: Path,
    output_path: Path,
    target_lang: str,
    backend: str = "deepseek",
    api_key: Optional[str] = None,
    model: str = "deepseek-chat",  # DeepSeek æ¨¡å‹åç§°
    prompt_template: str = DEFAULT_PROMPT,
    batch_size_chars: int = 30000,  # DeepSeek API limit: ~30K chars for stable processing
    max_items_per_batch: int = 200,  # Limit to 200 entries per batch for API stability
    verify_ssl: bool = True,
    whole_file: bool = False,  # æ˜¯å¦ä¸€æ¬¡æ€§æäº¤æ•´ä¸ªå­—å¹•æ–‡ä»¶
    merge_timelines: bool = False,  # æ˜¯å¦åˆå¹¶æ—¶é—´è½´
    min_duration_ms: int = 1000,  # æœ€çŸ­å­—å¹•æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºåˆå¹¶
):
    import time
    start_time = time.time()

    logger.info(f"è§£æ SRT æ–‡ä»¶: {input_path}")
    entries = parse_srt(input_path)
    if not entries:
        raise RuntimeError("No SRT entries parsed")

    # å¦‚æœéœ€è¦åˆå¹¶æ—¶é—´è½´ï¼Œå…ˆè¿›è¡Œåˆå¹¶
    if merge_timelines:
        logger.info("åˆå¹¶çŸ­æ—¶é—´è½´...")
        entries = merge_short_entries(entries, min_duration_ms)
        logger.info(f"åˆå¹¶åæ¡ç›®æ•°: {len(entries)}")

    logger.info(f"æ€»å­—å¹•æ¡ç›®: {len(entries)}")
    logger.info(f"åˆå§‹åŒ–ç¿»è¯‘å™¨: {backend}")

    translator = get_translator(name=backend, api_key=api_key, verify_ssl=verify_ssl, model=model)

    # å¦‚æœé€‰æ‹©ä¸€æ¬¡æ€§æäº¤æ•´ä¸ªæ–‡ä»¶
    if whole_file:
        logger.info("ä¸€æ¬¡æ€§æäº¤æ•´ä¸ªå­—å¹•æ–‡ä»¶è¿›è¡Œç¿»è¯‘ (åŸå§‹ SRT ä¸Šä¸‹æ–‡)")
        # ä½¿ç”¨ SRT ä¸“ç”¨æç¤ºè¯ï¼Œæ˜ç¡®å‘Šè¯‰æ¨¡å‹è¾“å…¥æ˜¯ä¸€ä¸ªå®Œæ•´ SRT æ–‡ä»¶å¹¶è¦æ±‚è¿”å›æœ‰æ•ˆ SRT
        subtitle_prompt = SUBTITLE_PROMPT_SRT if prompt_template == DEFAULT_PROMPT else prompt_template

        # è¯»å–åŸå§‹ srt æ–‡ä»¶æ–‡æœ¬ï¼ˆä¿æŒåŸæ ·ï¼Œä¸æ·»åŠ åºå·ï¼‰
        full_text = input_path.read_text(encoding="utf-8", errors="ignore")
        logger.info(f"å­—å¹•æ€»å­—ç¬¦æ•°: {len(full_text)}")

        # ä¸€æ¬¡æ€§ç¿»è¯‘
        logger.info("è°ƒç”¨ç¿»è¯‘ API...")
        translated_full = translator.translate(full_text, target_lang, subtitle_prompt)

        # ç®€å•æ ¡éªŒï¼šæ£€æŸ¥æ˜¯å¦åƒ SRTï¼ˆåŒ…å«æ—¶é—´è½´æ ‡è®°å’Œæ•°å­—ç´¢å¼•ï¼‰
        # looks_like_srt = ("-->" in translated_full) and (re.search(r'^\\s*\\d+\\s*$', translated_full, flags=re.M) is not None)
        looks_like_srt = True
        if looks_like_srt:
            logger.info("AI è¿”å›çœ‹èµ·æ¥åƒ SRTï¼Œç›´æ¥å†™å…¥è¾“å‡ºæ–‡ä»¶")
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text(translated_full.strip() + "\n", encoding="utf-8")
            total_time = time.time() - start_time
            logger.info(f"ç¿»è¯‘å®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}s")
            logger.info(f"å¹³å‡é€Ÿåº¦: {len(entries)/total_time:.1f} æ¡ç›®/ç§’")
            logger.info(f"API è°ƒç”¨æ¬¡æ•°: 1")
            return
        else:
            logger.warning("AI è¿”å›ä¸æ˜¯æ ‡å‡† SRTï¼Œå›é€€åˆ°åˆ†æ‰¹è§£æ/åˆ†å‰²é€»è¾‘...")
            # å¦‚æœæ ¡éªŒå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæœ‰çš„åˆ†å‰²/æ˜ å°„é€»è¾‘ä½œä¸ºå›é€€

    else:
        # åŸæœ‰çš„æ‰¹æ¬¡å¤„ç†é€»è¾‘
        logger.info(f"åˆ›å»ºæ‰¹æ¬¡ (æœ€å¤§ {batch_size_chars} å­—ç¬¦, {max_items_per_batch} æ¡ç›®/æ‰¹)")
        batches = batch_entries(entries, max_chars=batch_size_chars, max_items=max_items_per_batch)
        logger.info(f"æ€»æ‰¹æ¬¡æ•°: {len(batches)}")

        translated_texts: List[str] = []
        total_processed = 0

        for i, batch in enumerate(batches, 1):
            batch_start_time = time.time()

        # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡ä¿¡æ¯
        batch_chars = sum(len(e["text"]) for e in batch)
        print(f"\nğŸ”„ æ‰¹æ¬¡ {i}/{len(batches)} - {len(batch)} æ¡ç›® ({batch_chars} å­—ç¬¦)")

        # build batch text
        texts = [e["text"] for e in batch]
        batch_text = "\n".join(texts)

        # call translator
        print(f"ğŸ“¡ è°ƒç”¨ {backend} API...")
        translated = translator.translate(batch_text, target_lang, prompt_template)

        # map translated back to items
        parts = split_translated_text(translated, len(batch), batch)
        if len(parts) != len(batch):
            print(f"âš ï¸  ç¿»è¯‘ç»“æœæ•°é‡ä¸åŒ¹é…ï¼Œé‡è¯•åˆ†å‰²...")
            parts = split_translated_text(translated, len(batch), batch)

        for p in parts:
            translated_texts.append(p.strip())

        total_processed += len(batch)
        batch_time = time.time() - batch_start_time

        logger.info(f"æ‰¹æ¬¡ {i} å®Œæˆ - å¤„ç†äº† {len(batch)} æ¡ç›® (ç”¨æ—¶: {batch_time:.1f}s)")
        logger.info(f"æ€»è¿›åº¦: {total_processed}/{len(entries)} æ¡ç›® ({total_processed/len(entries)*100:.1f}%)")

    # å…³é—­æ‰¹æ¬¡å¤„ç†çš„ else å—
    if len(translated_texts) != len(entries):
        # safety: if mismatch, pad with empty strings
        # but better to raise so user notices
        raise RuntimeError(f"ç¿»è¯‘æ¡ç›®æ•°ä¸åŸæ¡ç›®æ•°ä¸åŒ¹é…: {len(translated_texts)} vs {len(entries)}")

    logger.info(f"æ„å»ºç¿»è¯‘åçš„å­—å¹•æ–‡ä»¶...")
    # build new entries
    new_entries = []
    for e, tr in zip(entries, translated_texts):
        new_entries.append({"index": e["index"], "start": e["start"], "end": e["end"], "text": tr})

    logger.info(f"å†™å…¥æ–‡ä»¶: {output_path}")
    write_srt(output_path, new_entries)

    total_time = time.time() - start_time
    logger.info(f"ç¿»è¯‘å®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}s")
    logger.info(f"å¹³å‡é€Ÿåº¦: {len(entries)/total_time:.1f} æ¡ç›®/ç§’")

    # Calculate API call count based on mode
    if whole_file:
        api_calls = 1
    else:
        api_calls = len(batches)
    logger.info(f"API è°ƒç”¨æ¬¡æ•°: {api_calls}")
